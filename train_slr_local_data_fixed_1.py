import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import cv2
import numpy as np
from PIL import Image
import os
import json
from tqdm import tqdm
import matplotlib.pyplot as plt
import pandas as pd  # ç”¨äºè¯»å–Excel

# --- é…ç½®éƒ¨åˆ† ---
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
VIDEO_DIR = os.path.join(PROJECT_ROOT, "videos")          # è§†é¢‘æ–‡ä»¶å­˜æ”¾ç›®å½•
ANNOTATION_FILE = os.path.join(PROJECT_ROOT, "csl_daily_data.xlsx") # Excel æ–‡ä»¶è·¯å¾„

# --- ä¼˜åŒ–: å¢åŠ  MAX_FRAMES ä»¥é€‚åº”æ•°æ®é›† ---
MAX_FRAMES = 450  # ğŸ‘ˆ ä¿®æ”¹: ä» 100 å¢åŠ åˆ° 450ï¼Œç¡®ä¿è¦†ç›–æ•°æ®é›†ä¸­æœ€é•¿çš„ gloss åºåˆ—
BATCH_SIZE = 2
NUM_EPOCHS = 30  # ğŸ‘ˆ ä¿®æ”¹: å‡å°‘è®­ç»ƒè½®æ•°è‡³30ï¼Œé¿å…è¿‡æ‹Ÿåˆ
LEARNING_RATE = 5e-4  # ğŸ‘ˆ ä¿®æ”¹: åˆå§‹å­¦ä¹ ç‡ï¼Œé…åˆOneCycleLR
TARGET_SIZE = (224, 224)
HIDDEN_SIZE = 128  # ğŸ‘ˆ ä¿®æ”¹: å‡å°‘éšè—å±‚å¤§å°
NUM_LAYERS = 1  # ğŸ‘ˆ ä¿®æ”¹: å‡å°‘LSTMå±‚æ•°
DROPOUT = 0.4  # ğŸ‘ˆ ä¿®æ”¹: å¢åŠ dropout
NUM_WORKERS = 4  # æ ¹æ® CPU æ ¸å¿ƒæ•°è°ƒæ•´
EARLY_STOP_PATIENCE = 8  # ğŸ‘ˆ ä¿®æ”¹: å‡å°‘æ—©åœè€å¿ƒå€¼ï¼Œé…åˆæ›´çŸ­çš„è®­ç»ƒå‘¨æœŸ
MIN_DELTA = 0.01  # æ—©åœæœ€å°æ”¹è¿›é˜ˆå€¼

# --- 1. è§£ææ ‡æ³¨æ–‡æ¡£å¹¶å»ºç«‹æ˜ å°„ ---
def parse_annotation_file(annotation_file_path):
    """
    è§£æ CSL-Daily çš„ Excel æ ‡æ³¨æ–‡ä»¶ï¼Œå¹¶å»ºç«‹ è§†é¢‘æ ‡è¯†ç¬¦ -> æ–‡æœ¬ çš„æ˜ å°„ã€‚
    ä» Excel æ–‡ä»¶çš„ 'name' åˆ—è·å– IDï¼Œä» 'gloss' åˆ—è·å–æ–‡æœ¬ã€‚
    """
    print(f"Parsing annotation file: {annotation_file_path}")
    df = pd.read_excel(annotation_file_path, engine='openpyxl')

    if 'name' not in df.columns or 'gloss' not in df.columns:
        raise ValueError(f"Excel file must contain 'name' and 'gloss' columns. Found columns: {list(df.columns)}")

    df_clean = df[['name', 'gloss']].dropna(subset=['name', 'gloss'])
    annotation_map = dict(zip(df_clean['name'], df_clean['gloss']))

    print(f"Successfully parsed {len(annotation_map)} unique annotations from Excel.")
    return annotation_map


# --- 2. æ„å»ºæœ¬åœ°æ•°æ®åˆ—è¡¨ (æ–°å¢: æ£€æŸ¥è§†é¢‘å¸§æ•°) ---
def build_local_data_list(video_dir, annotation_file_path, max_frames=MAX_FRAMES):
    """
    æ„å»ºæœ¬åœ°æ•°æ®åˆ—è¡¨ã€‚
    Args:
        video_dir (str): å­˜æ”¾æ‰€æœ‰ .mp4 è§†é¢‘æ–‡ä»¶çš„ç›®å½•ã€‚
        annotation_file_path (str): æ ‡æ³¨æ–‡ä»¶è·¯å¾„ (Excel)ã€‚
        max_frames (int): æœ€å¤§å…è®¸å¸§æ•°ã€‚
    Returns:
        data_list (list): åŒ…å«å­—å…¸çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å« 'video_path', 'text', 'video_id'ã€‚
    """
    print("Parsing annotations from Excel...")
    annotation_map = parse_annotation_file(annotation_file_path)

    video_files = [f for f in os.listdir(video_dir) if f.lower().endswith('.mp4')]
    print(f"Found {len(video_files)} video files in '{video_dir}'.")

    data_list = []
    missing_videos = 0
    invalid_videos = 0  # æ–°å¢è®¡æ•°å™¨

    for video_id in tqdm(annotation_map.keys(), desc="Building dataset from annotations"):
        text = annotation_map[video_id]

        expected_video_filename = f"{video_id}.mp4"
        expected_video_path = os.path.join(video_dir, expected_video_filename)

        if expected_video_filename in video_files:
            # --- æ–°å¢: æ£€æŸ¥è§†é¢‘æ˜¯å¦æœ‰æ•ˆä¸”å¸§æ•°åˆé€‚ ---
            cap = cv2.VideoCapture(expected_video_path)
            if not cap.isOpened():
                print(f"Warning: Failed to open video file '{expected_video_filename}' for ID '{video_id}'. Skipping.")
                invalid_videos += 1
                cap.release()
                continue
            
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            cap.release()

            if total_frames <= 0:
                print(f"Warning: Video file '{expected_video_filename}' has no frames for ID '{video_id}'. Skipping.")
                invalid_videos += 1
                continue
            
            if total_frames > max_frames:
                print(f"Warning: Video file '{expected_video_filename}' has {total_frames} frames, exceeding max_frames {max_frames} for ID '{video_id}'. Skipping.")
                invalid_videos += 1
                continue

            data_list.append({
                "video_path": expected_video_path,
                "text": text,
                "video_id": video_id
            })
        else:
            print(f"Warning: Video file '{expected_video_filename}' not found for ID '{video_id}'. Skipping.")
            missing_videos += 1

    print(f"Total valid samples built: {len(data_list)}")
    print(f"Samples without video: {missing_videos}")
    print(f"Samples with invalid/empty/long videos: {invalid_videos}")

    return data_list


# --- 3. æ•°æ®é›†ç±»å®šä¹‰ ---
class CSLDailyDataset(Dataset):
    def __init__(self, data_list, max_frames=MAX_FRAMES, target_size=(224, 224), gloss_to_id=None, is_train=False):
        self.data_list = data_list
        self.max_frames = max_frames
        self.target_size = target_size
        self.gloss_to_id = gloss_to_id
        self.is_train = is_train

        # ğŸ‘‰ å…³é”®ä¿®æ”¹ï¼šæ¢å¤ ToTensor()ï¼Œå¹¶ç¡®ä¿å…¶åœ¨ transform çš„å¼€å¤´
        if is_train:
            self.transform = transforms.Compose([
                transforms.Resize(target_size),
                # ğŸ‘ˆ å°† ToTensor() æ”¾åœ¨å‰é¢ï¼Œç¡®ä¿åç»­æ“ä½œæ¥æ”¶çš„æ˜¯ Tensor
                transforms.ToTensor(),
                # ğŸ‘ˆ å…¶ä»–å¢å¼ºæ“ä½œç°åœ¨æ¥æ”¶çš„æ˜¯ Tensor
                transforms.RandomApply([
                    transforms.RandomResizedCrop(target_size, scale=(0.75, 1.0))
                ], p=0.7),  # æé«˜æ¦‚ç‡
                transforms.RandomApply([
                    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.05)
                ], p=0.5),  # æé«˜æ¦‚ç‡
                transforms.RandomApply([
                    # ğŸ‘‰ æ–°å¢ï¼šéšæœºæ°´å¹³ç¿»è½¬ï¼ˆä»…å¯¹éæ‰‹è¯­å…³é”®å¸§ï¼‰
                    transforms.RandomHorizontalFlip(p=0.3)
                ], p=0.3),
                # ğŸ‘ˆ ç§»é™¤æœ‰é£é™©çš„ lambda å¢å¼ºï¼Œæˆ–ç”¨æ›´å®‰å…¨çš„æ–¹å¼æ›¿ä»£
                # transforms.RandomApply([
                #     lambda x: x + torch.randn_like(x) * 0.05
                # ], p=0.2),
                # ğŸ‘ˆ æœ€åè¿›è¡Œå½’ä¸€åŒ–
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        else:
            self.transform = transforms.Compose([
                transforms.Resize(target_size),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        item = self.data_list[idx]
        video_path = item.get('video_path', '')
        text = item.get('text', '')

        if isinstance(video_path, list):
            if len(video_path) > 0:
                video_path = video_path[0]
            else:
                raise ValueError(f"Empty list for video_path at index {idx}")
        elif not isinstance(video_path, str) or not video_path:
            raise ValueError(f"Invalid video_path at index {idx}: '{video_path}' (type: {type(video_path)})")

        if not os.path.exists(video_path):
            raise FileNotFoundError(f"Video file not found at index {idx}: '{video_path}'")

        # ğŸ‘‡ ä¸å†ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰å¸§ï¼Œè€Œæ˜¯è®°å½•è·¯å¾„å’Œå…ƒä¿¡æ¯
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"Warning: Failed to open video file at index {idx}: '{video_path}'. Returning empty frames.")
            processed_frames = torch.empty(0, 3, *self.target_size)
            gloss_tokens = []
            cap.release()
            return processed_frames, gloss_tokens

        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        cap.release()

        if total_frames <= 0:
            print(f"Warning: Video file '{video_path}' has no frames for ID '{video_path}'. Skipping.")
            return torch.empty(0, 3, *self.target_size), []

        # ğŸ‘‰ é™åˆ¶æœ€å¤§å¸§æ•°
        actual_frames = min(total_frames, self.max_frames)

        # ğŸ‘‰ åˆ›å»ºä¸€ä¸ªåˆ—è¡¨æ¥å­˜å‚¨å¸§ç´¢å¼•ï¼Œç”¨äºåç»­éšæœºé‡‡æ ·æˆ–å‡åŒ€é‡‡æ ·
        # å¦‚æœä¸éœ€è¦ temporal_stretchï¼Œå¯ä»¥ç›´æ¥ç”¨ range(actual_frames)
        frame_indices = list(range(actual_frames))

        # ğŸ‘‰ æ–°å¢ï¼šå¦‚æœæ˜¯è®­ç»ƒæ¨¡å¼ï¼Œå¯ä»¥åº”ç”¨æ—¶é—´æ‹‰ä¼¸ï¼ˆé€šè¿‡é‡é‡‡æ ·ç´¢å¼•å®ç°ï¼‰
        if self.is_train:
            stretch_factor = np.random.uniform(0.8, 1.2)
            new_length = int(len(frame_indices) * stretch_factor)
            if new_length > 0:
                indices = np.linspace(0, len(frame_indices) - 1, new_length).astype(int)
                frame_indices = [frame_indices[i] for i in indices]

        # ğŸ‘‰ å…³é”®ä¿®æ”¹ï¼šåªåŠ è½½éœ€è¦çš„å¸§
        frames = []
        cap = cv2.VideoCapture(video_path)
        for idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)  # è®¾ç½®åˆ°æŒ‡å®šå¸§
            ret, frame = cap.read()
            if not ret:
                break
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            # ğŸ‘ˆ åº”ç”¨ transform åˆ° numpy æ•°ç»„
            pil_image = Image.fromarray(frame_rgb)
            transformed_image = self.transform(pil_image)
            frames.append(transformed_image)
        cap.release()

        if len(frames) == 0:
            processed_frames = torch.empty(0, 3, *self.target_size)
        else:
            # ğŸ‘ˆ å°†åˆ—è¡¨ä¸­çš„ Tensor å †å æˆä¸€ä¸ªå¤§ Tensor
            processed_frames = torch.stack(frames)

        # å¤„ç†æ–‡æœ¬
        if isinstance(text, list):
            text = " ".join(str(item) for item in text)
        elif not isinstance(text, str):
            text = ""

        text = text.strip()
        gloss_tokens = [token for token in text.split() if token.strip()]

        # æ˜¾å¼åˆ é™¤ä¸´æ—¶å˜é‡
        del frames
        del frame_indices

        return processed_frames, gloss_tokens


# --- 4. æ¨¡å‹å®šä¹‰ ---
class LightweightSLRModel(nn.Module):
    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, dropout=DROPOUT):
        super(LightweightSLRModel, self).__init__()
        from torchvision.models import mobilenet_v2  # ğŸ‘ˆ ä¿®æ”¹ï¼šä½¿ç”¨MobileNetV2
        self.cnn = mobilenet_v2(weights="IMAGENET1K_V1").features  # ä»…ç‰¹å¾æå–å±‚
        # ğŸ‘ˆ å…³é”®ä¿®æ”¹ï¼šæ·»åŠ è‡ªé€‚åº”å¹³å‡æ± åŒ–å±‚ï¼Œå¼ºåˆ¶å°†ç‰¹å¾å›¾å‹ç¼©ä¸º (1, 1)
        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))
        # ğŸ‘ˆ å…³é”®ä¿®æ”¹ï¼šç§»é™¤å¯¹ cnn_features çš„ç¡¬ç¼–ç ï¼Œé€šè¿‡ä¸€æ¬¡å‰å‘ä¼ æ’­ç¡®å®šå…¶å®é™…å€¼
        # é€šè¿‡ä¸€æ¬¡å‰å‘ä¼ æ’­ç¡®å®šç‰¹å¾å‘é‡çš„é•¿åº¦
        dummy_input = torch.randn(1, 3, 224, 224)
        with torch.no_grad():
            cnn_out = self.cnn(dummy_input)
            cnn_out = self.adaptive_pool(cnn_out)
            self.cnn_features = cnn_out.view(1, -1).size(1)  # ğŸ‘ˆ åŠ¨æ€è·å–ç‰¹å¾é•¿åº¦

        # ğŸ‘ˆ ä¿®æ”¹ï¼šå¢åŠ CNNåDropout
        self.cnn_dropout = nn.Dropout(dropout * 0.5)  # è¾ƒå°çš„dropout

        self.lstm = nn.LSTM(
            input_size=self.cnn_features,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=False  # ğŸ‘ˆ ä¿®æ”¹ï¼šå•å‘LSTMï¼Œå‡å°‘å‚æ•°
        )
        self.lstm_output_size = hidden_size  # å•å‘LSTM

        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(self.lstm_output_size, num_classes)

    def forward(self, x):
        B, T, C, H, W = x.size()
        x = x.view(B * T, C, H, W)
        x = self.cnn(x)
        # ğŸ‘ˆ å…³é”®ä¿®å¤ï¼šä½¿ç”¨è‡ªé€‚åº”æ± åŒ–å±‚å°†ç‰¹å¾å›¾å‹ç¼©ä¸º (B*T, 1280, 1, 1)
        x = self.adaptive_pool(x)
        # ğŸ‘ˆ å…³é”®ä¿®å¤ï¼šå±•å¹³ç©ºé—´ç»´åº¦ï¼Œå¾—åˆ° (B*T, 1280)
        x = torch.flatten(x, start_dim=1)
        # ğŸ‘ˆ å…³é”®ä¿®å¤ï¼šé‡æ–° reshape ä¸º (B, T, 1280)
        x = x.view(B, T, -1)
        x = self.cnn_dropout(x)  # ğŸ‘ˆ æ–°å¢ï¼šCNNåDropout
        lstm_out, _ = self.lstm(x)
        logits = self.classifier(self.dropout(lstm_out))
        return logits  # [B, T, num_classes]

# --- 5. è¯æ±‡è¡¨æ„å»ºå‡½æ•° (æ–°å¢: æ•°æ®æ¸…æ´—) ---
def build_vocabulary(data_list, annotation_field='text'):
    """ä»æ•°æ®åˆ—è¡¨ä¸­æ„å»ºè¯æ±‡è¡¨"""
    unique_glosses = set()
    for item in data_list:
        gloss_str = item.get(annotation_field, '')
        gloss_str = gloss_str.strip()  # ğŸ‘ˆ æ–°å¢: æ¸…é™¤é¦–å°¾ç©ºç™½
        # ğŸ‘‡ ä¿®æ”¹: åˆ†å‰²å¹¶è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        gloss_tokens = [token for token in gloss_str.split() if token.strip()]
        unique_glosses.update(gloss_tokens)

    print(f"Found {len(unique_glosses)} unique glosses.")

    # ğŸ‘‰ å…³é”®ä¿®æ”¹ï¼šç¡®ä¿<blank>åœ¨é¦–ä½
    special_tokens = ["<blank>", "<pad>", "<sos>", "<eos>", "<unk>"]
    all_glosses_list = special_tokens + sorted(list(unique_glosses))

    gloss_to_id = {gloss: idx for idx, gloss in enumerate(all_glosses_list)}
    id_to_gloss = {idx: gloss for gloss, idx in gloss_to_id.items()}

    print("Vocabulary built.")
    return gloss_to_id, id_to_gloss


# --- 6. è‡ªå®šä¹‰æ‰¹æ¬¡åˆå¹¶å‡½æ•° (é€‚é… CTC Loss) ---
def collate_fn(batch, gloss_to_id):
    """è‡ªå®šä¹‰æ‰¹æ¬¡åˆå¹¶å‡½æ•°ï¼Œå¤„ç†ä¸åŒé•¿åº¦çš„è§†é¢‘å’Œæ ‡ç­¾ï¼Œç”¨äº CTC Lossã€‚"""
    frames_batch = [item[0] for item in batch]
    glosses_batch = [item[1] for item in batch]

    # å¡«å……å¸§åˆ°æœ€å¤§é•¿åº¦
    max_t = max(f.size(0) for f in frames_batch)
    padded_frames_batch = []
    input_lengths = []
    for frames in frames_batch:
        t, c, h, w = frames.size()
        input_lengths.append(t)
        if t < max_t:
            pad_size = max_t - t
            padding = torch.zeros(pad_size, c, h, w, dtype=frames.dtype, device=frames.device)
            frames_padded = torch.cat([frames, padding], dim=0)
        else:
            frames_padded = frames[:max_t]
        padded_frames_batch.append(frames_padded)
    padded_frames_batch = torch.stack(padded_frames_batch)

    # è½¬æ¢glossä¸ºIDå¹¶å¤„ç†ç”¨äº CTC
    targets = []
    target_lengths = []
    for glosses in glosses_batch:
        if not isinstance(glosses, list):
            glosses = []

        # å°†glosså­—ç¬¦ä¸²åˆ—è¡¨è½¬æ¢ä¸ºIDåˆ—è¡¨ï¼Œè·³è¿‡ <pad>
        ids = []
        for gloss in glosses:
            # ğŸ‘‰ ä½¿ç”¨<blank>è€Œä¸æ˜¯<pad>ä½œä¸ºCTCçš„blank
            token_id = gloss_to_id.get(gloss, gloss_to_id.get('<unk>', gloss_to_id['<unk>']))
            if token_id != gloss_to_id.get('<pad>', gloss_to_id['<pad>']):
                ids.append(token_id)
        targets.extend(ids)
        target_lengths.append(len(ids))

    targets = torch.tensor(targets, dtype=torch.long)
    target_lengths = torch.tensor(target_lengths, dtype=torch.long)
    input_lengths = torch.tensor(input_lengths, dtype=torch.long)

    return padded_frames_batch, targets, input_lengths, target_lengths

# --- 7. æ ‡ç­¾å¹³æ»‘CTCæŸå¤±å‡½æ•° ---
def label_smoothing_ctc_loss(log_probs, targets, input_lengths, target_lengths, smoothing=0.1, blank_idx=0):
    """
    å®ç°å¸¦æ ‡ç­¾å¹³æ»‘çš„CTCæŸå¤±
    """
    # è®¡ç®—åŸå§‹CTCæŸå¤±
    ctc_loss = nn.CTCLoss(blank=blank_idx, zero_infinity=True, reduction='none')
    loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)

    # è®¡ç®—æ ‡ç­¾å¹³æ»‘é¡¹
    # å¹³æ»‘ç›®æ ‡ï¼šå¤§éƒ¨åˆ†æ¦‚ç‡åˆ†é…ç»™æ­£ç¡®æ ‡ç­¾ï¼Œå°éƒ¨åˆ†å‡åŒ€åˆ†é…ç»™å…¶ä»–æ ‡ç­¾
    vocab_size = log_probs.size(-1)
    smooth_target = torch.ones_like(log_probs) / vocab_size  # å‡åŒ€åˆ†å¸ƒ

    # ğŸ‘ˆ ä¿®å¤ï¼šåˆ›å»ºä¸€ä¸ªä¸ log_probs å½¢çŠ¶ç›¸åŒçš„å¼ é‡æ¥å¡«å……ç›®æ ‡ä½ç½®
    # æˆ‘ä»¬éœ€è¦å°†ä¸€ç»´çš„ targets æ˜ å°„å› [T, B] çš„äºŒç»´ç»“æ„
    batch_size = log_probs.size(1)
    max_time = log_probs.size(0)

    # åˆ›å»ºä¸€ä¸ªå…¨é›¶çš„äºŒç»´å¼ é‡ [T, B]
    target_2d = torch.zeros(max_time, batch_size, dtype=torch.long, device=log_probs.device)

    # æ ¹æ® target_lengths å°† targets å¡«å…¥ target_2d
    current_idx = 0
    for i in range(batch_size):
        length = target_lengths[i].item()
        if length > 0:
            target_2d[:length, i] = targets[current_idx:current_idx + length]
            current_idx += length

    # ä½¿ç”¨ gather æ¥æ­£ç¡®åœ°åœ¨ç›®æ ‡ä½ç½®åˆ†é…æ¦‚ç‡
    # è¿™é‡Œæˆ‘ä»¬ç›´æ¥åœ¨ [T, B] ä¸Šæ“ä½œï¼Œç„¶åæ‰©å±•åˆ° [T, B, vocab_size]
    # æ›´ç®€å•çš„æ–¹å¼ï¼šå¯¹æ¯ä¸ªæ—¶é—´æ­¥ï¼Œè®¡ç®—è¯¥æ—¶é—´æ­¥ä¸Šæ‰€æœ‰æ ·æœ¬çš„ç›®æ ‡ä½ç½®
    # æˆ‘ä»¬å¯ä»¥è¿™æ ·æ“ä½œï¼šå¯¹äºæ¯ä¸ªæ—¶é—´æ­¥å’Œæ¯ä¸ªæ ·æœ¬ï¼Œå°†æ­£ç¡®æ ‡ç­¾çš„æ¦‚ç‡è®¾ä¸º (1-smoothing)
    # æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å®ç°ï¼š
    # æ–¹æ³•äºŒï¼šé€ä¸ªæ ·æœ¬å¤„ç†
    # è¿™ç§æ–¹æ³•è™½ç„¶æ•ˆç‡ç¨ä½ï¼Œä½†é€»è¾‘æ¸…æ™°ï¼Œä¸æ˜“å‡ºé”™
    # æˆ‘ä»¬ä¸å†ä½¿ç”¨ scatter_ï¼Œè€Œæ˜¯æ‰‹åŠ¨æ„é€ å¹³æ»‘ç›®æ ‡
    # ä½†è¿™ä¼šå¾ˆæ…¢ï¼Œæ‰€ä»¥è¿˜æ˜¯å›åˆ° scatter_ çš„æ€è·¯ï¼Œä½†ç¡®ä¿ç»´åº¦æ­£ç¡®

    # ä¿®æ­£åçš„ scatter_ ç”¨æ³•ï¼š
    # æˆ‘ä»¬éœ€è¦ä¸€ä¸ªç´¢å¼•å¼ é‡ï¼Œå…¶å½¢çŠ¶ä¸º [T, B, 1]ï¼Œå†…å®¹æ˜¯æ¯ä¸ªä½ç½®çš„ç›®æ ‡æ ‡ç­¾
    # æˆ‘ä»¬å·²ç»æœ‰äº† target_2d_expanded
    # ç°åœ¨ï¼Œæˆ‘ä»¬ç”¨å®ƒæ¥æ›´æ–° smooth_target
    # ä½† smooth_target æ˜¯ [T, B, vocab_size]ï¼Œæˆ‘ä»¬éœ€è¦åœ¨ç¬¬2ç»´ï¼ˆvocab_size ç»´åº¦ï¼‰ä¸Šè¿›è¡Œ scatter
    # æ­£ç¡®çš„åšæ³•æ˜¯ï¼š
    # smooth_target.scatter_(2, target_2d_expanded, 1 - smoothing)
    # ä½†æ˜¯ï¼Œè¿™è¦æ±‚ target_2d_expanded çš„å½¢çŠ¶æ˜¯ [T, B, 1]ï¼Œå¹¶ä¸”å®ƒçš„å€¼æ˜¯ç±»åˆ«ç´¢å¼•
    # è¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„

    # ğŸ‘ˆ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„ç´¢å¼•è¿›è¡Œ scatter
    # ç¡®ä¿ target_2d_expanded çš„å½¢çŠ¶æ˜¯ [T, B, 1]
    # ç„¶ååœ¨ç¬¬2ç»´è¿›è¡Œ scatter
    target_2d_expanded = target_2d.unsqueeze(-1)  # [T, B, 1]
    smooth_target.scatter_(2, target_2d_expanded, 1 - smoothing)

    # ä½¿ç”¨äº¤å‰ç†µä½œä¸ºå¹³æ»‘é¡¹
    log_probs_flat = log_probs.permute(1, 0, 2).contiguous().view(-1, vocab_size)  # [T*B, vocab_size]
    smooth_target_flat = smooth_target.permute(1, 0, 2).contiguous().view(-1, vocab_size)  # [T*B, vocab_size]
    ce_loss = -torch.sum(smooth_target_flat * log_probs_flat, dim=1)  # [T*B]

    # å¹³å‡æŸå¤±
    ce_loss = ce_loss.mean()

    # ç»“åˆCTCæŸå¤±å’Œå¹³æ»‘é¡¹
    combined_loss = (1 - smoothing) * loss.mean() + smoothing * ce_loss

    return combined_loss

# --- 8. ä¸»è®­ç»ƒå‡½æ•° ---
def train_model():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    try:
        import openpyxl
    except ImportError:
        print("Installing 'openpyxl' library...")
        os.system("pip install openpyxl")
    try:
        import pandas
    except ImportError:
        print("Installing 'pandas' library...")
        os.system("pip install pandas")

    print("\n=== Step 1: Building Local Data List ===")
    # ğŸ‘ˆ è°ƒç”¨ä¿®æ”¹åçš„å‡½æ•°ï¼Œä¼ å…¥ MAX_FRAMES
    data_list = build_local_data_list(VIDEO_DIR, ANNOTATION_FILE, max_frames=MAX_FRAMES)

    print("\n=== Step 2: Splitting Data into Train/Val ===")
    split_index = int(0.9 * len(data_list))
    train_data_list = data_list[:split_index]
    val_data_list = data_list[split_index:]

    print(f"Training set size: {len(train_data_list)}")
    print(f"Validation set size: {len(val_data_list)}")

    print("\n=== Step 3: Building Vocabulary ===")
    # ğŸ‘ˆ ä½¿ç”¨ä¿®æ”¹åçš„ build_vocabulary å‡½æ•°
    gloss_to_id, id_to_gloss = build_vocabulary(data_list, annotation_field='text')
    vocab_size = len(gloss_to_id)
    print(f"Vocabulary size: {vocab_size}")

    with open('gloss_to_id.json', 'w', encoding='utf-8') as f:
        json.dump(gloss_to_id, f, ensure_ascii=False, indent=2)
    with open('id_to_gloss.json', 'w', encoding='utf-8') as f:
        json.dump(id_to_gloss, f, ensure_ascii=False, indent=2)

    print("\n=== Step 4: Creating Data Loaders ===")
    train_dataset = CSLDailyDataset(data_list=train_data_list, max_frames=MAX_FRAMES, target_size=TARGET_SIZE, gloss_to_id=gloss_to_id, is_train=True)
    val_dataset = CSLDailyDataset(data_list=val_data_list, max_frames=MAX_FRAMES, target_size=TARGET_SIZE, gloss_to_id=gloss_to_id, is_train=False)

    def train_collate_fn(batch):
        return collate_fn(batch, gloss_to_id)

    def val_collate_fn(batch):
        return collate_fn(batch, gloss_to_id)

    # ğŸ‘ˆ å…³é”®ä¿®æ”¹ï¼šå¢åŠ  prefetch_factor ä»¥æé«˜æ•°æ®é¢„å–æ•ˆç‡
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn=train_collate_fn, prefetch_factor=2)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=val_collate_fn, prefetch_factor=2)

    print("\n=== Step 5: Initializing Model ===")
    model = LightweightSLRModel(num_classes=vocab_size, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, dropout=DROPOUT)
    model.to(device)

    # ğŸ‘‰ å…³é”®ä¿®æ”¹ï¼šç¡®ä¿ blank_idx ä¸º0ï¼ˆ<blank>çš„ç´¢å¼•ï¼‰
    blank_idx = gloss_to_id['<blank>']
    assert blank_idx == 0, f"Expected blank_idx to be 0, got {blank_idx}. Check vocabulary order."

    # ğŸ‘ˆ ä¿®æ”¹ï¼šä½¿ç”¨AdamWä¼˜åŒ–å™¨
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)

    # ğŸ‘ˆ æ–°å¢ï¼šå­¦ä¹ ç‡è°ƒåº¦å™¨ - è°ƒæ•´å‚æ•°
    scheduler = OneCycleLR(
        optimizer,
        max_lr=LEARNING_RATE,
        steps_per_epoch=len(train_loader),
        epochs=NUM_EPOCHS,  # ğŸ‘ˆ ç°åœ¨ä½¿ç”¨æ–°çš„NUM_EPOCHS=30
        pct_start=0.2,  # ğŸ‘ˆ å¢åŠ é¢„çƒ­é˜¶æ®µåˆ°20%
        anneal_strategy='cos',
        div_factor=25.0,  # ğŸ‘ˆ å¢å¤§åˆå§‹å­¦ä¹ ç‡ä¸æœ€å¤§å­¦ä¹ ç‡çš„æ¯”å€¼
        final_div_factor=1e4  # ğŸ‘ˆ æœ€ç»ˆå­¦ä¹ ç‡æ›´å°
    )

    print("\n=== Step 6: Starting Training ===")
    model.train()
    train_losses = []
    val_losses = []

    # ğŸ‘ˆ æ–°å¢ï¼šæ—©åœæœºåˆ¶
    best_val_loss = float('inf')
    patience_counter = 0

    for epoch in range(NUM_EPOCHS): # ğŸ‘ˆ ç°åœ¨æ˜¯30è½®
        print(f"\nEpoch {epoch+1}/{NUM_EPOCHS}")
        running_loss = 0.0
        progress_bar = tqdm(train_loader, desc=f"Training Epoch {epoch+1}")

        for batch_idx, (batch_frames, batch_targets, batch_input_lengths, batch_target_lengths) in enumerate(progress_bar):
            batch_frames = batch_frames.to(device)
            batch_targets = batch_targets.to(device)
            batch_input_lengths = batch_input_lengths.to(device)
            batch_target_lengths = batch_target_lengths.to(device)

            optimizer.zero_grad()

            outputs = model(batch_frames)  # [B, T, num_classes]
            log_probs = torch.log_softmax(outputs, dim=2).permute(1, 0, 2)  # [T, B, num_classes]

            # ğŸ‘ˆ ä½¿ç”¨æ ‡ç­¾å¹³æ»‘CTCæŸå¤±
            loss = label_smoothing_ctc_loss(
                log_probs,
                batch_targets,
                batch_input_lengths,
                batch_target_lengths,
                smoothing=0.1,  # æ ‡ç­¾å¹³æ»‘å‚æ•°
                blank_idx=blank_idx
            )

            loss.backward()

            # ğŸ‘ˆ æ–°å¢ï¼šæ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()
            scheduler.step()  # ğŸ‘ˆ æ–°å¢ï¼šè°ƒåº¦å™¨æ­¥è¿›

            loss_value = loss.item()
            running_loss += loss_value
            progress_bar.set_postfix(loss=loss_value)

        avg_loss = running_loss / len(train_loader)
        print(f"Average Training Loss: {avg_loss:.4f}")
        train_losses.append(avg_loss)

        # --- éªŒè¯ ---
        model.eval()
        val_running_loss = 0.0
        with torch.no_grad():
            val_progress_bar = tqdm(val_loader, desc="Validation")
            for val_batch_idx, (val_batch_frames, val_batch_targets, val_batch_input_lengths, val_batch_target_lengths) in enumerate(val_progress_bar):
                val_batch_frames = val_batch_frames.to(device)
                val_batch_targets = val_batch_targets.to(device)
                val_batch_input_lengths = val_batch_input_lengths.to(device)
                val_batch_target_lengths = val_batch_target_lengths.to(device)

                val_outputs = model(val_batch_frames)  # [B, T, num_classes]
                val_log_probs = torch.log_softmax(val_outputs, dim=2).permute(1, 0, 2)  # [T, B, num_classes]

                # éªŒè¯æ—¶ä¹Ÿä½¿ç”¨æ ‡ç­¾å¹³æ»‘æŸå¤±ï¼Œä½†å¯ä»¥ä¸ä½¿ç”¨æ ‡ç­¾å¹³æ»‘ä»¥è·å¾—çœŸå®æ€§èƒ½
                val_loss = nn.CTCLoss(blank=blank_idx, zero_infinity=True, reduction='mean')(
                    val_log_probs, val_batch_targets, val_batch_input_lengths, val_batch_target_lengths
                )

                val_running_loss += val_loss.item()

        avg_val_loss = val_running_loss / len(val_loader)
        print(f"Average Validation Loss: {avg_val_loss:.4f}")
        val_losses.append(avg_val_loss)

        # ğŸ‘ˆ æ”¹è¿›çš„æ—©åœæ£€æŸ¥
        if avg_val_loss < best_val_loss - MIN_DELTA:
            best_val_loss = avg_val_loss
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': avg_loss,
                'val_loss': avg_val_loss,
            }, 'best_model.pth')
            print(f"New best model saved with validation loss: {avg_val_loss:.4f}")
        else:
            patience_counter += 1
            if patience_counter >= EARLY_STOP_PATIENCE:
                print(f"Early stopping triggered after {epoch+1} epochs. No improvement for {EARLY_STOP_PATIENCE} epochs.")
                break

        # ğŸ‘‰ é¢å¤–æ£€æŸ¥ï¼šå¦‚æœè®­ç»ƒæŸå¤±è¿œä½äºéªŒè¯æŸå¤±ä¸”éªŒè¯æŸå¤±ä¸å†ä¸‹é™ï¼Œæå‰åœæ­¢
        if avg_loss < 0.5 and avg_val_loss > 3.0 and epoch > 15: # 15æ˜¯30çš„ä¸€åŠ
            print(f"Training loss too low ({avg_loss}) compared to validation loss ({avg_val_loss}). Possible overfitting. Stopping early.")
            break

        model.train()

    print("\n=== Step 7: Plotting and Saving ===")
    import numpy as np
    train_losses_np = np.array(train_losses, dtype=np.float32)
    val_losses_np = np.array(val_losses, dtype=np.float32)

    plt.figure(figsize=(10, 5))
    plt.plot(range(1, len(train_losses_np) + 1), train_losses_np, label='Training Loss', marker='o')
    if len(val_losses_np) > 0:
        plt.plot(range(1, len(val_losses_np) + 1), val_losses_np, label='Validation Loss', marker='s')
    plt.title('Model Training Loss (CTC with Label Smoothing) - 30 Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig('training_loss_plot_ctc_fixed.png')
    plt.show()

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(model.state_dict(), 'slr_model_ctc_fixed_final.pth')
    print("Final model saved as 'slr_model_ctc_fixed_final.pth'.")
    print(f"Best model saved as 'best_model.pth' with validation loss: {best_val_loss:.4f}")


if __name__ == "__main__":
    if not os.path.exists(VIDEO_DIR):
        print(f"Error: Video directory '{VIDEO_DIR}' does not exist.")
        print("Please create a folder named 'videos' in the project root and place your .mp4 files inside it.")
        exit(1)

    if not os.path.exists(ANNOTATION_FILE):
        print(f"Error: Annotation file '{ANNOTATION_FILE}' does not exist.")
        print("Please ensure the Excel file is present.")
        exit(1)

    local_dataset_dir = "local_csl_daily"
    if os.path.exists(local_dataset_dir):
        print(f"Removing old dataset cache: {local_dataset_dir}")
        import shutil
        shutil.rmtree(local_dataset_dir)

    train_model()